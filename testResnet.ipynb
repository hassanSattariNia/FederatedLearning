{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RESNET 18 IMPORT DIRECTLY"
      ],
      "metadata": {
        "id": "IHT9pgwoP4Rm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gvAsH6730WyG",
        "outputId": "3a8e3d88-42a4-4e6f-9869-5898b6160949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10\n",
            "Batch 100, Loss: 2.070\n",
            "Batch 200, Loss: 1.753\n",
            "Batch 300, Loss: 1.606\n",
            "Batch 400, Loss: 1.573\n",
            "Batch 500, Loss: 1.517\n",
            "Batch 600, Loss: 1.463\n",
            "Batch 700, Loss: 1.424\n",
            "Epoch 1 finished with accuracy: 50.26%\n",
            "Epoch 2/10\n",
            "Batch 100, Loss: 1.324\n",
            "Batch 200, Loss: 1.313\n",
            "Batch 300, Loss: 1.292\n",
            "Batch 400, Loss: 1.269\n",
            "Batch 500, Loss: 1.246\n",
            "Batch 600, Loss: 1.248\n",
            "Batch 700, Loss: 1.247\n",
            "Epoch 2 finished with accuracy: 56.18%\n",
            "Epoch 3/10\n",
            "Batch 100, Loss: 1.128\n",
            "Batch 200, Loss: 1.123\n",
            "Batch 300, Loss: 1.107\n",
            "Batch 400, Loss: 1.110\n",
            "Batch 500, Loss: 1.077\n",
            "Batch 600, Loss: 1.090\n",
            "Batch 700, Loss: 1.079\n",
            "Epoch 3 finished with accuracy: 60.45%\n",
            "Epoch 4/10\n",
            "Batch 100, Loss: 0.992\n",
            "Batch 200, Loss: 1.011\n",
            "Batch 300, Loss: 0.969\n",
            "Batch 400, Loss: 0.983\n",
            "Batch 500, Loss: 0.971\n",
            "Batch 600, Loss: 0.982\n",
            "Batch 700, Loss: 0.956\n",
            "Epoch 4 finished with accuracy: 62.78%\n",
            "Epoch 5/10\n",
            "Batch 100, Loss: 0.882\n",
            "Batch 200, Loss: 0.878\n",
            "Batch 300, Loss: 0.889\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e43f059cbd08>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0mdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m     \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mWELCOME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend_bytes\u001b[0;34m(self, buf, offset, size)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"buffer length < offset + size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# تنظیمات اولیه و انتقال به GPU اگر ممکن است\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# بارگذاری و پیش‌پردازش CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# بارگذاری مدل ResNet-18 و تغییر آخرین لایه به تعداد دسته‌های CIFAR-10\n",
        "model = resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)  # چون CIFAR-10 ده دسته دارد\n",
        "model = model.to(device)\n",
        "\n",
        "# تعریف تابع از دست دادن و بهینه‌ساز\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# تابع محاسبه دقت (accuracy) در دیتاست تست\n",
        "def calculate_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# آموزش مدل\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"Batch {i + 1}, Loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # ارزیابی مدل و محاسبه دقت در دیتاست تست\n",
        "    model.eval()\n",
        "    accuracy = calculate_accuracy(model, testloader)\n",
        "    print(f\"Epoch {epoch + 1} finished with accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print('آموزش به پایان رسید')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET 18 FROM SCRATCH"
      ],
      "metadata": {
        "id": "71WuqhXzP9-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# تعریف بلوک پایه‌ای ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "# تعریف مدل ResNet\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = nn.functional.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# ساخت مدل ResNet-18\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "# تنظیمات اولیه و انتقال به GPU اگر ممکن است\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet18().to(device)\n",
        "\n",
        "# بارگذاری و پیش‌پردازش CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# تعریف تابع از دست دادن و بهینه‌ساز\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# تابع محاسبه دقت (accuracy) در دیتاست تست\n",
        "def calculate_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# آموزش مدل\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"Batch {i + 1}, Loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # ارزیابی مدل و محاسبه دقت در دیتاست تست\n",
        "    model.eval()\n",
        "    accuracy = calculate_accuracy(model, testloader)\n",
        "    print(f\"Epoch {epoch + 1} finished with accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print('آموزش به پایان رسید')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Ph-k79R00ru",
        "outputId": "0f4a8002-1ef1-421a-a45a-937d424464f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10\n",
            "Batch 100, Loss: 1.959\n",
            "Batch 200, Loss: 1.557\n",
            "Batch 300, Loss: 1.415\n",
            "Batch 400, Loss: 1.354\n",
            "Batch 500, Loss: 1.280\n",
            "Batch 600, Loss: 1.196\n",
            "Batch 700, Loss: 1.125\n",
            "Epoch 1 finished with accuracy: 61.37%\n",
            "Epoch 2/10\n",
            "Batch 100, Loss: 1.025\n",
            "Batch 200, Loss: 0.991\n",
            "Batch 300, Loss: 0.937\n",
            "Batch 400, Loss: 0.926\n",
            "Batch 500, Loss: 0.901\n",
            "Batch 600, Loss: 0.876\n",
            "Batch 700, Loss: 0.859\n",
            "Epoch 2 finished with accuracy: 71.05%\n",
            "Epoch 3/10\n",
            "Batch 100, Loss: 0.728\n",
            "Batch 200, Loss: 0.708\n",
            "Batch 300, Loss: 0.713\n",
            "Batch 400, Loss: 0.699\n",
            "Batch 500, Loss: 0.682\n",
            "Batch 600, Loss: 0.684\n",
            "Batch 700, Loss: 0.695\n",
            "Epoch 3 finished with accuracy: 73.50%\n",
            "Epoch 4/10\n",
            "Batch 100, Loss: 0.593\n",
            "Batch 200, Loss: 0.580\n",
            "Batch 300, Loss: 0.551\n",
            "Batch 400, Loss: 0.576\n",
            "Batch 500, Loss: 0.569\n",
            "Batch 600, Loss: 0.569\n",
            "Batch 700, Loss: 0.552\n",
            "Epoch 4 finished with accuracy: 77.35%\n",
            "Epoch 5/10\n",
            "Batch 100, Loss: 0.445\n",
            "Batch 200, Loss: 0.465\n",
            "Batch 300, Loss: 0.450\n",
            "Batch 400, Loss: 0.477\n",
            "Batch 500, Loss: 0.451\n",
            "Batch 600, Loss: 0.460\n",
            "Batch 700, Loss: 0.466\n",
            "Epoch 5 finished with accuracy: 78.00%\n",
            "Epoch 6/10\n",
            "Batch 100, Loss: 0.378\n",
            "Batch 200, Loss: 0.379\n",
            "Batch 300, Loss: 0.370\n",
            "Batch 400, Loss: 0.382\n",
            "Batch 500, Loss: 0.381\n",
            "Batch 600, Loss: 0.370\n",
            "Batch 700, Loss: 0.377\n",
            "Epoch 6 finished with accuracy: 79.32%\n",
            "Epoch 7/10\n",
            "Batch 100, Loss: 0.286\n",
            "Batch 200, Loss: 0.295\n",
            "Batch 300, Loss: 0.318\n",
            "Batch 400, Loss: 0.308\n",
            "Batch 500, Loss: 0.315\n",
            "Batch 600, Loss: 0.324\n",
            "Batch 700, Loss: 0.322\n",
            "Epoch 7 finished with accuracy: 79.03%\n",
            "Epoch 8/10\n",
            "Batch 100, Loss: 0.235\n",
            "Batch 200, Loss: 0.249\n",
            "Batch 300, Loss: 0.240\n",
            "Batch 400, Loss: 0.239\n",
            "Batch 500, Loss: 0.248\n",
            "Batch 600, Loss: 0.272\n",
            "Batch 700, Loss: 0.263\n",
            "Epoch 8 finished with accuracy: 80.97%\n",
            "Epoch 9/10\n",
            "Batch 100, Loss: 0.195\n",
            "Batch 200, Loss: 0.199\n",
            "Batch 300, Loss: 0.196\n",
            "Batch 400, Loss: 0.200\n",
            "Batch 500, Loss: 0.208\n",
            "Batch 600, Loss: 0.198\n",
            "Batch 700, Loss: 0.234\n",
            "Epoch 9 finished with accuracy: 79.41%\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b2ba13e337da>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {i + 1}, Loss: {running_loss / 100:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET 18 WITH AUTO FORWARD AND BACKWARD PROPAGATION"
      ],
      "metadata": {
        "id": "xIH-Te64QEVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "filename = \"multi_part_resnet_18.txt\"\n",
        "with open(filename, \"w\") as file:\n",
        "  file.write(\"accuracies values\")\n",
        "# تنظیمات اولیه و انتقال به GPU اگر ممکن است\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# تعریف بلوک پایه‌ای ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "# پارتیشن‌های مدل\n",
        "class Partition1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Partition1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        return out\n",
        "\n",
        "class Partition2(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Partition2, self).__init__()\n",
        "        self.layer1 = self._make_layer(BasicBlock, in_channels, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 64, 128, 2, stride=2)\n",
        "\n",
        "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        return out\n",
        "\n",
        "class Partition3(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Partition3, self).__init__()\n",
        "        self.layer3 = self._make_layer(BasicBlock, in_channels, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 256, 512, 2, stride=2)\n",
        "\n",
        "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer3(x)\n",
        "        out = self.layer4(out)\n",
        "        return out\n",
        "\n",
        "class Partition4(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=10):\n",
        "        super(Partition4, self).__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.avgpool(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# ایجاد پارتیشن‌ها\n",
        "partition1 = Partition1().to(device)\n",
        "partition2 = Partition2(64).to(device)\n",
        "partition3 = Partition3(128).to(device)\n",
        "partition4 = Partition4(512).to(device)\n",
        "\n",
        "# تابع انتقال بین پارتیشن‌ها\n",
        "def forward_distributed(x):\n",
        "    x = partition1(x)\n",
        "    x = partition2(x)\n",
        "    x = partition3(x)\n",
        "    x = partition4(x)\n",
        "    return x\n",
        "\n",
        "# بارگذاری و پیش‌پردازش CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# تعریف تابع از دست دادن و بهینه‌ساز\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(partition1.parameters()) + list(partition2.parameters()) +\n",
        "                      list(partition3.parameters()) + list(partition4.parameters()), lr=0.001, momentum=0.9)\n",
        "\n",
        "# تابع محاسبه دقت (accuracy) در دیتاست تست\n",
        "def calculate_accuracy():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = forward_distributed(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# آموزش مدل\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = forward_distributed(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"Batch {i + 1}, Loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # ارزیابی مدل و محاسبه دقت در دیتاست تست\n",
        "    accuracy = calculate_accuracy()\n",
        "    print(f\"Epoch {epoch + 1} finished with accuracy: {accuracy:.2f}%\")\n",
        "    file.write(f\"{epoch +1} - ${accuracy:.2f}\")\n",
        "print('آموزش به پایان رسید')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW3NkbGz2Uuu",
        "outputId": "451b4b19-cbc5-4935-a951-2ef2ce156924"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10\n",
            "Batch 100, Loss: 1.933\n",
            "Batch 200, Loss: 1.562\n",
            "Batch 300, Loss: 1.406\n",
            "Batch 400, Loss: 1.319\n",
            "Batch 500, Loss: 1.248\n",
            "Batch 600, Loss: 1.175\n",
            "Batch 700, Loss: 1.122\n",
            "Epoch 1 finished with accuracy: 63.01%\n",
            "Epoch 2/10\n",
            "Batch 100, Loss: 0.983\n",
            "Batch 200, Loss: 0.943\n",
            "Batch 300, Loss: 0.917\n",
            "Batch 400, Loss: 0.895\n",
            "Batch 500, Loss: 0.897\n",
            "Batch 600, Loss: 0.824\n",
            "Batch 700, Loss: 0.836\n",
            "Epoch 2 finished with accuracy: 70.54%\n",
            "Epoch 3/10\n",
            "Batch 100, Loss: 0.743\n",
            "Batch 200, Loss: 0.708\n",
            "Batch 300, Loss: 0.682\n",
            "Batch 400, Loss: 0.676\n",
            "Batch 500, Loss: 0.681\n",
            "Batch 600, Loss: 0.679\n",
            "Batch 700, Loss: 0.656\n",
            "Epoch 3 finished with accuracy: 75.63%\n",
            "Epoch 4/10\n",
            "Batch 100, Loss: 0.557\n",
            "Batch 200, Loss: 0.534\n",
            "Batch 300, Loss: 0.546\n",
            "Batch 400, Loss: 0.570\n",
            "Batch 500, Loss: 0.550\n",
            "Batch 600, Loss: 0.581\n",
            "Batch 700, Loss: 0.539\n",
            "Epoch 4 finished with accuracy: 77.53%\n",
            "Epoch 5/10\n",
            "Batch 100, Loss: 0.458\n",
            "Batch 200, Loss: 0.477\n",
            "Batch 300, Loss: 0.455\n",
            "Batch 400, Loss: 0.441\n",
            "Batch 500, Loss: 0.451\n",
            "Batch 600, Loss: 0.442\n",
            "Batch 700, Loss: 0.441\n",
            "Epoch 5 finished with accuracy: 79.35%\n",
            "Epoch 6/10\n",
            "Batch 100, Loss: 0.352\n",
            "Batch 200, Loss: 0.353\n",
            "Batch 300, Loss: 0.385\n",
            "Batch 400, Loss: 0.384\n",
            "Batch 500, Loss: 0.383\n",
            "Batch 600, Loss: 0.384\n",
            "Batch 700, Loss: 0.384\n",
            "Epoch 6 finished with accuracy: 80.26%\n",
            "Epoch 7/10\n",
            "Batch 100, Loss: 0.276\n",
            "Batch 200, Loss: 0.281\n",
            "Batch 300, Loss: 0.307\n",
            "Batch 400, Loss: 0.300\n",
            "Batch 500, Loss: 0.320\n",
            "Batch 600, Loss: 0.311\n",
            "Batch 700, Loss: 0.319\n",
            "Epoch 7 finished with accuracy: 79.56%\n",
            "Epoch 8/10\n",
            "Batch 100, Loss: 0.248\n",
            "Batch 200, Loss: 0.233\n",
            "Batch 300, Loss: 0.239\n",
            "Batch 400, Loss: 0.261\n",
            "Batch 500, Loss: 0.251\n",
            "Batch 600, Loss: 0.264\n",
            "Batch 700, Loss: 0.269\n",
            "Epoch 8 finished with accuracy: 81.83%\n",
            "Epoch 9/10\n",
            "Batch 100, Loss: 0.186\n",
            "Batch 200, Loss: 0.189\n",
            "Batch 300, Loss: 0.182\n",
            "Batch 400, Loss: 0.208\n",
            "Batch 500, Loss: 0.190\n",
            "Batch 600, Loss: 0.225\n",
            "Batch 700, Loss: 0.211\n",
            "Epoch 9 finished with accuracy: 80.86%\n",
            "Epoch 10/10\n",
            "Batch 100, Loss: 0.159\n",
            "Batch 200, Loss: 0.147\n",
            "Batch 300, Loss: 0.162\n",
            "Batch 400, Loss: 0.150\n",
            "Batch 500, Loss: 0.161\n",
            "Batch 600, Loss: 0.176\n",
            "Batch 700, Loss: 0.191\n",
            "Epoch 10 finished with accuracy: 82.32%\n",
            "آموزش به پایان رسید\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# تنظیمات اولیه و انتقال به GPU اگر ممکن است\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# تعریف بلوک پایه‌ای ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "# پارتیشن‌های مدل\n",
        "class Partition1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Partition1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        return out\n",
        "\n",
        "class Partition2(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Partition2, self).__init__()\n",
        "        self.layer1 = self._make_layer(BasicBlock, in_channels, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 64, 128, 2, stride=2)\n",
        "\n",
        "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        return out\n",
        "\n",
        "class Partition3(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Partition3, self).__init__()\n",
        "        self.layer3 = self._make_layer(BasicBlock, in_channels, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 256, 512, 2, stride=2)\n",
        "\n",
        "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer3(x)\n",
        "        out = self.layer4(out)\n",
        "        return out\n",
        "\n",
        "class Partition4(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=10):\n",
        "        super(Partition4, self).__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.avgpool(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# ایجاد پارتیشن‌ها\n",
        "partition1 = Partition1().to(device)\n",
        "partition2 = Partition2(64).to(device)\n",
        "partition3 = Partition3(128).to(device)\n",
        "partition4 = Partition4(512).to(device)\n",
        "\n",
        "# بارگذاری و پیش‌پردازش CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# تعریف تابع از دست دادن و بهینه‌ساز\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(partition1.parameters()) + list(partition2.parameters()) +\n",
        "                      list(partition3.parameters()) + list(partition4.parameters()), lr=0.001, momentum=0.9)\n",
        "\n",
        "# شبیه‌سازی فرآیند forward و backward با متغیرهای لوکال\n",
        "for epoch in range(1):  # یک epoch برای تست\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # Forward pass با متغیرهای لوکال\n",
        "        output_layer1 = partition1(inputs)\n",
        "        output_layer2 = partition2(output_layer1.detach())\n",
        "        output_layer3 = partition3(output_layer2.detach())\n",
        "        output_layer4 = partition4(output_layer3.detach())\n",
        "\n",
        "        # محاسبه Loss\n",
        "        loss = criterion(output_layer4, labels)\n",
        "\n",
        "        # Backward pass به صورت دستی\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # backward برای مرحله آخر\n",
        "        output_layer4.backward(torch.ones_like(output_layer4), retain_graph=True)\n",
        "\n",
        "        # backward برای مراحل میانی به صورت دستی\n",
        "        output_layer3.backward(torch.ones_like(output_layer3), retain_graph=True)\n",
        "        output_layer2.backward(torch.ones_like(output_layer2), retain_graph=True)\n",
        "        output_layer1.backward(torch.ones_like(output_layer1))\n",
        "\n",
        "        # به‌روزرسانی وزن‌ها\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Batch {i}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwGlozsz6miR",
        "outputId": "d4ee4cbf-0806-4aa7-abde-a12259c140b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Batch 0, Loss: 2.3641\n",
            "Batch 100, Loss: 2.2966\n",
            "Batch 200, Loss: 2.3044\n",
            "Batch 300, Loss: 2.3019\n",
            "Batch 400, Loss: 2.3020\n",
            "Batch 500, Loss: 2.3012\n",
            "Batch 600, Loss: 2.2998\n",
            "Batch 700, Loss: 2.3019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code with custom forward and backward"
      ],
      "metadata": {
        "id": "S4SsEqjMIb6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESNET-18 WITH CUSTOM FORWARD AND BACKWARD PROPATION"
      ],
      "metadata": {
        "id": "VS9oI25pQOrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# تنظیمات اولیه و انتقال به GPU اگر ممکن است\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "list_accuracies = []\n",
        "\n",
        "\n",
        "# تعریف بلوک پایه‌ای ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "# پارتیشن‌های مدل\n",
        "class Partition1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Partition1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        return out\n",
        "\n",
        "class Partition2(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Partition2, self).__init__()\n",
        "        self.layer1 = self._make_layer(BasicBlock, in_channels, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 64, 128, 2, stride=2)\n",
        "\n",
        "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        return out\n",
        "\n",
        "class Partition3(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Partition3, self).__init__()\n",
        "        self.layer3 = self._make_layer(BasicBlock, in_channels, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 256, 512, 2, stride=2)\n",
        "\n",
        "    def _make_layer(self, block, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer3(x)\n",
        "        out = self.layer4(out)\n",
        "        return out\n",
        "\n",
        "class Partition4(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=10):\n",
        "        super(Partition4, self).__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.avgpool(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# ایجاد پارتیشن‌ها\n",
        "partition1 = Partition1().to(device)\n",
        "partition2 = Partition2(64).to(device)\n",
        "partition3 = Partition3(128).to(device)\n",
        "partition4 = Partition4(512).to(device)\n",
        "\n",
        "# بارگذاری و پیش‌پردازش CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# تعریف تابع از دست دادن و بهینه‌ساز\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(partition1.parameters()) + list(partition2.parameters()) +\n",
        "                      list(partition3.parameters()) + list(partition4.parameters()), lr=0.01, momentum=0.9)\n",
        "\n",
        "# تابع محاسبه دقت (accuracy) در دیتاست تست\n",
        "def calculate_accuracy():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            output_layer1 = partition1(images)\n",
        "            output_layer2 = partition2(output_layer1)\n",
        "            output_layer3 = partition3(output_layer2)\n",
        "            output_layer4 = partition4(output_layer3)\n",
        "            _, predicted = torch.max(output_layer4, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# آموزش مدل\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # Forward pass بدون .detach()\n",
        "        output_layer1 = partition1(inputs)\n",
        "        output_layer2 = partition2(output_layer1)\n",
        "        output_layer3 = partition3(output_layer2)\n",
        "        output_layer4 = partition4(output_layer3)\n",
        "\n",
        "        # محاسبه Loss\n",
        "        loss = criterion(output_layer4, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"Batch {i + 1}, Loss: {running_loss / 100:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # ارزیابی مدل و محاسبه دقت در دیتاست تست\n",
        "    accuracy = calculate_accuracy()\n",
        "    print(f\"Epoch {epoch + 1} finished with accuracy: {accuracy:.2f}%\")\n",
        "    list_accuracies.append(accuracy)\n",
        "print('آموزش به پایان رسید')\n",
        "\n",
        "\n",
        "\n",
        "filename = \"multi_part_resnet18.txt\"\n",
        "# Open the file in write mode\n",
        "with open(filename, \"w\") as file:\n",
        "  file.write(f'accuracies: {list_accuracies}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs5v802J9iKM",
        "outputId": "5151e004-5c71-4a8f-9a61-f18abfdb566c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/5\n",
            "Batch 100, Loss: 1.8725\n",
            "Batch 200, Loss: 1.5423\n",
            "Batch 300, Loss: 1.3653\n",
            "Batch 400, Loss: 1.2401\n",
            "Batch 500, Loss: 1.1528\n",
            "Batch 600, Loss: 1.0644\n",
            "Batch 700, Loss: 0.9978\n",
            "Epoch 1 finished with accuracy: 65.56%\n",
            "Epoch 2/5\n",
            "Batch 100, Loss: 0.8432\n",
            "Batch 200, Loss: 0.7799\n",
            "Batch 300, Loss: 0.7822\n",
            "Batch 400, Loss: 0.7575\n",
            "Batch 500, Loss: 0.7300\n",
            "Batch 600, Loss: 0.7097\n",
            "Batch 700, Loss: 0.6991\n",
            "Epoch 2 finished with accuracy: 76.17%\n",
            "Epoch 3/5\n",
            "Batch 100, Loss: 0.5960\n",
            "Batch 200, Loss: 0.6353\n",
            "Batch 300, Loss: 0.5930\n",
            "Batch 400, Loss: 0.5867\n",
            "Batch 500, Loss: 0.5796\n",
            "Batch 600, Loss: 0.5495\n",
            "Batch 700, Loss: 0.5443\n",
            "Epoch 3 finished with accuracy: 80.95%\n",
            "Epoch 4/5\n",
            "Batch 100, Loss: 0.4675\n",
            "Batch 200, Loss: 0.4665\n",
            "Batch 300, Loss: 0.4705\n",
            "Batch 400, Loss: 0.4517\n",
            "Batch 500, Loss: 0.4797\n",
            "Batch 600, Loss: 0.4663\n",
            "Batch 700, Loss: 0.4279\n",
            "Epoch 4 finished with accuracy: 80.38%\n",
            "Epoch 5/5\n",
            "Batch 100, Loss: 0.4042\n",
            "Batch 200, Loss: 0.3765\n",
            "Batch 300, Loss: 0.3932\n",
            "Batch 400, Loss: 0.3846\n",
            "Batch 500, Loss: 0.4155\n",
            "Batch 600, Loss: 0.3909\n",
            "Batch 700, Loss: 0.4074\n",
            "Epoch 5 finished with accuracy: 83.76%\n",
            "آموزش به پایان رسید\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqmWlC2A_6rl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kj1WJY3oO8BB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}