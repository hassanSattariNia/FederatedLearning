{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIEf2jH8yapaD2DD3gNW0b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ce7ad0c3f0f461b8985e909b601cd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6611c0d6bb7424aa14e96b6b6542072",
              "IPY_MODEL_d9d2ab99abec4f74873b23c3ecc1c059",
              "IPY_MODEL_5e47517a5e7c48ddb47f6f6ee772acbe"
            ],
            "layout": "IPY_MODEL_13dd2e707d164fadbfb16389be5f17c1"
          }
        },
        "a6611c0d6bb7424aa14e96b6b6542072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5dd1abb472346c089bef36b8e8198a1",
            "placeholder": "​",
            "style": "IPY_MODEL_f1875218bd364b259b9b3bebf5f911f0",
            "value": "config.json: 100%"
          }
        },
        "d9d2ab99abec4f74873b23c3ecc1c059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a048b25dc54fc3acd679f9beb1d9d0",
            "max": 684,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cf9390180b447d9bef5dc2e0067f565",
            "value": 684
          }
        },
        "5e47517a5e7c48ddb47f6f6ee772acbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab96956eeac4242971a99ee17bc571f",
            "placeholder": "​",
            "style": "IPY_MODEL_a9b452e8717d4c8bb6a85e2fb3de3e82",
            "value": " 684/684 [00:00&lt;00:00, 25.6kB/s]"
          }
        },
        "13dd2e707d164fadbfb16389be5f17c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5dd1abb472346c089bef36b8e8198a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1875218bd364b259b9b3bebf5f911f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a048b25dc54fc3acd679f9beb1d9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf9390180b447d9bef5dc2e0067f565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ab96956eeac4242971a99ee17bc571f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b452e8717d4c8bb6a85e2fb3de3e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e877a0a55a934d6bb2591e6a4420ddf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dec0fc19b46440ebd5116ba9f9e27fb",
              "IPY_MODEL_0745880e27bb4c429892be0df16b1cc4",
              "IPY_MODEL_2452acd313984eb092e7981986eaebec"
            ],
            "layout": "IPY_MODEL_964e86dd75644aafaa03feaca1aa73c6"
          }
        },
        "8dec0fc19b46440ebd5116ba9f9e27fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c777013166bd451ca45815411933c9fd",
            "placeholder": "​",
            "style": "IPY_MODEL_d14a0482db444cd6b69f1122a423819c",
            "value": "model.safetensors: 100%"
          }
        },
        "0745880e27bb4c429892be0df16b1cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7fbfa6d46314593847f15a378711bb9",
            "max": 47372894,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe1e00cdc08148119a5f9393c79f8acc",
            "value": 47372894
          }
        },
        "2452acd313984eb092e7981986eaebec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dae3972c61c14884b28cc9f646ebbdfa",
            "placeholder": "​",
            "style": "IPY_MODEL_0e78a61b1e5c4ec18d7db77a240a060d",
            "value": " 47.4M/47.4M [00:00&lt;00:00, 82.6MB/s]"
          }
        },
        "964e86dd75644aafaa03feaca1aa73c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c777013166bd451ca45815411933c9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d14a0482db444cd6b69f1122a423819c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7fbfa6d46314593847f15a378711bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe1e00cdc08148119a5f9393c79f8acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dae3972c61c14884b28cc9f646ebbdfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e78a61b1e5c4ec18d7db77a240a060d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassanSattariNia/FederatedLearning/blob/main/spliteTo8Client.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4ce7ad0c3f0f461b8985e909b601cd08",
            "a6611c0d6bb7424aa14e96b6b6542072",
            "d9d2ab99abec4f74873b23c3ecc1c059",
            "5e47517a5e7c48ddb47f6f6ee772acbe",
            "13dd2e707d164fadbfb16389be5f17c1",
            "f5dd1abb472346c089bef36b8e8198a1",
            "f1875218bd364b259b9b3bebf5f911f0",
            "68a048b25dc54fc3acd679f9beb1d9d0",
            "4cf9390180b447d9bef5dc2e0067f565",
            "7ab96956eeac4242971a99ee17bc571f",
            "a9b452e8717d4c8bb6a85e2fb3de3e82",
            "e877a0a55a934d6bb2591e6a4420ddf1",
            "8dec0fc19b46440ebd5116ba9f9e27fb",
            "0745880e27bb4c429892be0df16b1cc4",
            "2452acd313984eb092e7981986eaebec",
            "964e86dd75644aafaa03feaca1aa73c6",
            "c777013166bd451ca45815411933c9fd",
            "d14a0482db444cd6b69f1122a423819c",
            "b7fbfa6d46314593847f15a378711bb9",
            "fe1e00cdc08148119a5f9393c79f8acc",
            "dae3972c61c14884b28cc9f646ebbdfa",
            "0e78a61b1e5c4ec18d7db77a240a060d"
          ]
        },
        "id": "fRup2bRyf6dt",
        "outputId": "b5401b93-7d51-4480-e43c-b0afbb25e566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ce7ad0c3f0f461b8985e909b601cd08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e877a0a55a934d6bb2591e6a4420ddf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ALBERT Model Analysis ===\n",
            "Total Parameters: 11,683,584\n",
            "Total Size in MB: 44.58\n",
            "\n",
            "=== Layer-wise Analysis ===\n",
            "\n",
            "Layer: embeddings.word_embeddings\n",
            "Type: Embedding\n",
            "Parameters: 3,840,000\n",
            "Memory (MB): 14.65\n",
            "\n",
            "Layer: embeddings.position_embeddings\n",
            "Type: Embedding\n",
            "Parameters: 65,536\n",
            "Memory (MB): 0.25\n",
            "\n",
            "Layer: embeddings.token_type_embeddings\n",
            "Type: Embedding\n",
            "Parameters: 256\n",
            "Memory (MB): 0.00\n",
            "\n",
            "Layer: embeddings.LayerNorm\n",
            "Type: LayerNorm\n",
            "Parameters: 256\n",
            "Memory (MB): 0.00\n",
            "\n",
            "Layer: encoder.embedding_hidden_mapping_in\n",
            "Type: Linear\n",
            "Parameters: 99,072\n",
            "Memory (MB): 0.38\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm\n",
            "Type: LayerNorm\n",
            "Parameters: 1,536\n",
            "Memory (MB): 0.01\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.attention.query\n",
            "Type: Linear\n",
            "Parameters: 590,592\n",
            "Memory (MB): 2.25\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.attention.key\n",
            "Type: Linear\n",
            "Parameters: 590,592\n",
            "Memory (MB): 2.25\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.attention.value\n",
            "Type: Linear\n",
            "Parameters: 590,592\n",
            "Memory (MB): 2.25\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.attention.dense\n",
            "Type: Linear\n",
            "Parameters: 590,592\n",
            "Memory (MB): 2.25\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm\n",
            "Type: LayerNorm\n",
            "Parameters: 1,536\n",
            "Memory (MB): 0.01\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.ffn\n",
            "Type: Linear\n",
            "Parameters: 2,362,368\n",
            "Memory (MB): 9.01\n",
            "\n",
            "Layer: encoder.albert_layer_groups.0.albert_layers.0.ffn_output\n",
            "Type: Linear\n",
            "Parameters: 2,360,064\n",
            "Memory (MB): 9.00\n",
            "\n",
            "Layer: pooler\n",
            "Type: Linear\n",
            "Parameters: 590,592\n",
            "Memory (MB): 2.25\n",
            "\n",
            "=== Model Configuration ===\n",
            "Hidden Size: 768\n",
            "Intermediate Size: 3072\n",
            "Number of Hidden Layers: 12\n",
            "Number of Attention Heads: 12\n",
            "\n",
            "=== Suggested Partition Points (4 devices) ===\n",
            "Partition 1: Cut after embeddings.word_embeddings\n",
            "Partition 2: Cut after encoder.albert_layer_groups.0.albert_layers.0.ffn\n",
            "Partition 3: Cut after pooler\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AlbertModel, AlbertConfig\n",
        "import numpy as np\n",
        "\n",
        "def analyze_albert_structure():\n",
        "    # Load ALBERT model\n",
        "    model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "\n",
        "    # Get total number of parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Analyze memory requirements\n",
        "    param_size = 0\n",
        "    buffer_size = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_in_mb = (param_size + buffer_size) / 1024**2\n",
        "\n",
        "    # Analyze layer structure\n",
        "    layer_info = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if len(list(module.children())) == 0:  # If it's a leaf module\n",
        "            num_params = sum(p.numel() for p in module.parameters())\n",
        "            layer_info[name] = {\n",
        "                'parameters': num_params,\n",
        "                'memory_mb': (num_params * 4) / (1024**2),  # Assuming float32\n",
        "                'type': module.__class__.__name__\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        'total_parameters': total_params,\n",
        "        'total_size_mb': size_in_mb,\n",
        "        'layer_info': layer_info,\n",
        "        'config': model.config,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "def print_model_analysis(analysis):\n",
        "    print(\"\\n=== ALBERT Model Analysis ===\")\n",
        "    print(f\"Total Parameters: {analysis['total_parameters']:,}\")\n",
        "    print(f\"Total Size in MB: {analysis['total_size_mb']:.2f}\")\n",
        "\n",
        "    print(\"\\n=== Layer-wise Analysis ===\")\n",
        "    for name, info in analysis['layer_info'].items():\n",
        "        if info['parameters'] > 0:  # Only show layers with parameters\n",
        "            print(f\"\\nLayer: {name}\")\n",
        "            print(f\"Type: {info['type']}\")\n",
        "            print(f\"Parameters: {info['parameters']:,}\")\n",
        "            print(f\"Memory (MB): {info['memory_mb']:.2f}\")\n",
        "\n",
        "    print(\"\\n=== Model Configuration ===\")\n",
        "    config = analysis['config']\n",
        "    print(f\"Hidden Size: {config.hidden_size}\")\n",
        "    print(f\"Intermediate Size: {config.intermediate_size}\")\n",
        "    print(f\"Number of Hidden Layers: {config.num_hidden_layers}\")\n",
        "    print(f\"Number of Attention Heads: {config.num_attention_heads}\")\n",
        "\n",
        "# Function to identify potential partition points\n",
        "def suggest_partition_points(analysis, num_devices):\n",
        "    total_params = analysis['total_parameters']\n",
        "    target_size = total_params / num_devices\n",
        "\n",
        "    current_size = 0\n",
        "    partition_suggestions = []\n",
        "\n",
        "    for name, info in analysis['layer_info'].items():\n",
        "        current_size += info['parameters']\n",
        "        if current_size >= target_size:\n",
        "            partition_suggestions.append(name)\n",
        "            current_size = 0\n",
        "\n",
        "    return partition_suggestions\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Analyze model\n",
        "    analysis = analyze_albert_structure()\n",
        "    print_model_analysis(analysis)\n",
        "\n",
        "    # Example: Suggest partition points for 4 devices\n",
        "    print(\"\\n=== Suggested Partition Points (4 devices) ===\")\n",
        "    partition_points = suggest_partition_points(analysis, 4)\n",
        "    for i, point in enumerate(partition_points, 1):\n",
        "        print(f\"Partition {i}: Cut after {point}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AlbertModel\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class LayerProfile:\n",
        "    name: str\n",
        "    parameters: int\n",
        "    memory_mb: float\n",
        "    flops: int\n",
        "    critical_path: bool\n",
        "    dependencies: List[str]\n",
        "\n",
        "class AlbertPartitioner:\n",
        "    def __init__(self, num_clients=8):\n",
        "        self.num_clients = num_clients\n",
        "        self.model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "        self.config = self.model.config\n",
        "\n",
        "    def profile_layers(self) -> Dict[str, LayerProfile]:\n",
        "        layers = {}\n",
        "        sequence_length = 512  # Standard sequence length\n",
        "\n",
        "        # Profile embeddings\n",
        "        embed_params = sum(p.numel() for p in self.model.embeddings.parameters())\n",
        "        embed_flops = self.config.embedding_size * sequence_length\n",
        "        layers['embeddings'] = LayerProfile(\n",
        "            name='embeddings',\n",
        "            parameters=embed_params,\n",
        "            memory_mb=embed_params * 4 / (1024**2),\n",
        "            flops=embed_flops,\n",
        "            critical_path=True,\n",
        "            dependencies=[]\n",
        "        )\n",
        "\n",
        "        # Profile each transformer layer\n",
        "        for i in range(self.config.num_hidden_layers):\n",
        "            # Attention layer\n",
        "            attention_params = sum(p.numel() for p in\n",
        "                                self.model.encoder.albert_layer_groups[0].albert_layers[0].attention.parameters())\n",
        "            attention_flops = (sequence_length ** 2) * self.config.hidden_size * 4\n",
        "\n",
        "            layers[f'attention_{i}'] = LayerProfile(\n",
        "                name=f'attention_{i}',\n",
        "                parameters=attention_params,\n",
        "                memory_mb=attention_params * 4 / (1024**2),\n",
        "                flops=attention_flops,\n",
        "                critical_path=True,\n",
        "                dependencies=[f'embeddings'] if i == 0 else [f'ffn_{i-1}']\n",
        "            )\n",
        "\n",
        "            # FFN layer\n",
        "            ffn_params = sum(p.numel() for p in\n",
        "                           self.model.encoder.albert_layer_groups[0].albert_layers[0].ffn.parameters())\n",
        "            ffn_flops = sequence_length * self.config.hidden_size * self.config.intermediate_size * 2\n",
        "\n",
        "            layers[f'ffn_{i}'] = LayerProfile(\n",
        "                name=f'ffn_{i}',\n",
        "                parameters=ffn_params,\n",
        "                memory_mb=ffn_params * 4 / (1024**2),\n",
        "                flops=ffn_flops,\n",
        "                critical_path=True,\n",
        "                dependencies=[f'attention_{i}']\n",
        "            )\n",
        "\n",
        "        # Profile pooler\n",
        "        pooler_params = sum(p.numel() for p in self.model.pooler.parameters())\n",
        "        pooler_flops = self.config.hidden_size ** 2\n",
        "        layers['pooler'] = LayerProfile(\n",
        "            name='pooler',\n",
        "            parameters=pooler_params,\n",
        "            memory_mb=pooler_params * 4 / (1024**2),\n",
        "            flops=pooler_flops,\n",
        "            critical_path=True,\n",
        "            dependencies=[f'ffn_{self.config.num_hidden_layers-1}']\n",
        "        )\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def create_optimal_partitions(self) -> List[Dict]:\n",
        "        layers = self.profile_layers()\n",
        "\n",
        "        # Strategy for 8 clients:\n",
        "        # 1. Client 0: Embeddings (heavy memory, low compute)\n",
        "        # 2-6. Clients 1-5: 2-3 transformer layers each (balanced compute)\n",
        "        # 7. Client 6: Remaining transformer layers\n",
        "        # 8. Client 7: Pooler and final operations\n",
        "\n",
        "        partitions = [{\n",
        "            'client_id': i,\n",
        "            'layers': [],\n",
        "            'total_params': 0,\n",
        "            'total_flops': 0,\n",
        "            'memory_mb': 0.0,\n",
        "            'dependencies': set()\n",
        "        } for i in range(self.num_clients)]\n",
        "\n",
        "        # Assign embeddings to first client\n",
        "        partitions[0]['layers'].append(layers['embeddings'])\n",
        "        partitions[0]['total_params'] += layers['embeddings'].parameters\n",
        "        partitions[0]['total_flops'] += layers['embeddings'].flops\n",
        "        partitions[0]['memory_mb'] += layers['embeddings'].memory_mb\n",
        "\n",
        "        # Distribute transformer layers\n",
        "        transformer_layers = [(k, v) for k, v in layers.items()\n",
        "                            if 'attention' in k or 'ffn' in k]\n",
        "        layers_per_client = len(transformer_layers) // (self.num_clients - 2)\n",
        "\n",
        "        for i, (name, layer) in enumerate(transformer_layers):\n",
        "            client_id = 1 + (i // layers_per_client)\n",
        "            if client_id >= self.num_clients - 1:\n",
        "                client_id = self.num_clients - 2\n",
        "\n",
        "            partitions[client_id]['layers'].append(layer)\n",
        "            partitions[client_id]['total_params'] += layer.parameters\n",
        "            partitions[client_id]['total_flops'] += layer.flops\n",
        "            partitions[client_id]['memory_mb'] += layer.memory_mb\n",
        "            partitions[client_id]['dependencies'].update(layer.dependencies)\n",
        "\n",
        "        # Assign pooler to last client\n",
        "        partitions[-1]['layers'].append(layers['pooler'])\n",
        "        partitions[-1]['total_params'] += layers['pooler'].parameters\n",
        "        partitions[-1]['total_flops'] += layers['pooler'].flops\n",
        "        partitions[-1]['memory_mb'] += layers['pooler'].memory_mb\n",
        "        partitions[-1]['dependencies'].update(layers['pooler'].dependencies)\n",
        "\n",
        "        return partitions\n",
        "\n",
        "def print_partition_analysis(partitions):\n",
        "    print(\"\\n=== 8-Client Partition Analysis ===\")\n",
        "    total_params = sum(p['total_params'] for p in partitions)\n",
        "    total_flops = sum(p['total_flops'] for p in partitions)\n",
        "    total_memory = sum(p['memory_mb'] for p in partitions)\n",
        "\n",
        "    print(f\"\\nTotal Model Statistics:\")\n",
        "    print(f\"Total Parameters: {total_params:,}\")\n",
        "    print(f\"Total Estimated FLOPs: {total_flops:,}\")\n",
        "    print(f\"Total Memory Usage: {total_memory:.2f} MB\")\n",
        "\n",
        "    for partition in partitions:\n",
        "        print(f\"\\nClient {partition['client_id']}:\")\n",
        "        print(f\"Parameters: {partition['total_params']:,} ({partition['total_params']/total_params*100:.1f}%)\")\n",
        "        print(f\"FLOPs: {partition['total_flops']:,} ({partition['total_flops']/total_flops*100:.1f}%)\")\n",
        "        print(f\"Memory: {partition['memory_mb']:.2f} MB ({partition['memory_mb']/total_memory*100:.1f}%)\")\n",
        "        print(\"Layers:\")\n",
        "        for layer in partition['layers']:\n",
        "            print(f\"  - {layer.name}\")\n",
        "            if partition['dependencies']:\n",
        "                print(f\"    Dependencies: {partition['dependencies']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    partitioner = AlbertPartitioner(num_clients=8)\n",
        "    partitions = partitioner.create_optimal_partitions()\n",
        "    print_partition_analysis(partitions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26ofyLH7jquC",
        "outputId": "58c1c51b-1d4a-4089-b095-980862c3792b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 8-Client Partition Analysis ===\n",
            "\n",
            "Total Model Statistics:\n",
            "Total Parameters: 61,211,904\n",
            "Total Estimated FLOPs: 38,655,361,024\n",
            "Total Memory Usage: 233.50 MB\n",
            "\n",
            "Client 0:\n",
            "Parameters: 3,906,048 (6.4%)\n",
            "FLOPs: 65,536 (0.0%)\n",
            "Memory: 14.90 MB (6.4%)\n",
            "Layers:\n",
            "  - embeddings\n",
            "\n",
            "Client 1:\n",
            "Parameters: 9,452,544 (15.4%)\n",
            "FLOPs: 6,442,450,944 (16.7%)\n",
            "Memory: 36.06 MB (15.4%)\n",
            "Layers:\n",
            "  - attention_0\n",
            "    Dependencies: {'ffn_0', 'attention_1', 'embeddings', 'attention_0'}\n",
            "  - ffn_0\n",
            "    Dependencies: {'ffn_0', 'attention_1', 'embeddings', 'attention_0'}\n",
            "  - attention_1\n",
            "    Dependencies: {'ffn_0', 'attention_1', 'embeddings', 'attention_0'}\n",
            "  - ffn_1\n",
            "    Dependencies: {'ffn_0', 'attention_1', 'embeddings', 'attention_0'}\n",
            "\n",
            "Client 2:\n",
            "Parameters: 9,452,544 (15.4%)\n",
            "FLOPs: 6,442,450,944 (16.7%)\n",
            "Memory: 36.06 MB (15.4%)\n",
            "Layers:\n",
            "  - attention_2\n",
            "    Dependencies: {'ffn_2', 'attention_2', 'ffn_1', 'attention_3'}\n",
            "  - ffn_2\n",
            "    Dependencies: {'ffn_2', 'attention_2', 'ffn_1', 'attention_3'}\n",
            "  - attention_3\n",
            "    Dependencies: {'ffn_2', 'attention_2', 'ffn_1', 'attention_3'}\n",
            "  - ffn_3\n",
            "    Dependencies: {'ffn_2', 'attention_2', 'ffn_1', 'attention_3'}\n",
            "\n",
            "Client 3:\n",
            "Parameters: 9,452,544 (15.4%)\n",
            "FLOPs: 6,442,450,944 (16.7%)\n",
            "Memory: 36.06 MB (15.4%)\n",
            "Layers:\n",
            "  - attention_4\n",
            "    Dependencies: {'ffn_3', 'attention_4', 'attention_5', 'ffn_4'}\n",
            "  - ffn_4\n",
            "    Dependencies: {'ffn_3', 'attention_4', 'attention_5', 'ffn_4'}\n",
            "  - attention_5\n",
            "    Dependencies: {'ffn_3', 'attention_4', 'attention_5', 'ffn_4'}\n",
            "  - ffn_5\n",
            "    Dependencies: {'ffn_3', 'attention_4', 'attention_5', 'ffn_4'}\n",
            "\n",
            "Client 4:\n",
            "Parameters: 9,452,544 (15.4%)\n",
            "FLOPs: 6,442,450,944 (16.7%)\n",
            "Memory: 36.06 MB (15.4%)\n",
            "Layers:\n",
            "  - attention_6\n",
            "    Dependencies: {'ffn_6', 'attention_6', 'ffn_5', 'attention_7'}\n",
            "  - ffn_6\n",
            "    Dependencies: {'ffn_6', 'attention_6', 'ffn_5', 'attention_7'}\n",
            "  - attention_7\n",
            "    Dependencies: {'ffn_6', 'attention_6', 'ffn_5', 'attention_7'}\n",
            "  - ffn_7\n",
            "    Dependencies: {'ffn_6', 'attention_6', 'ffn_5', 'attention_7'}\n",
            "\n",
            "Client 5:\n",
            "Parameters: 9,452,544 (15.4%)\n",
            "FLOPs: 6,442,450,944 (16.7%)\n",
            "Memory: 36.06 MB (15.4%)\n",
            "Layers:\n",
            "  - attention_8\n",
            "    Dependencies: {'ffn_8', 'attention_8', 'attention_9', 'ffn_7'}\n",
            "  - ffn_8\n",
            "    Dependencies: {'ffn_8', 'attention_8', 'attention_9', 'ffn_7'}\n",
            "  - attention_9\n",
            "    Dependencies: {'ffn_8', 'attention_8', 'attention_9', 'ffn_7'}\n",
            "  - ffn_9\n",
            "    Dependencies: {'ffn_8', 'attention_8', 'attention_9', 'ffn_7'}\n",
            "\n",
            "Client 6:\n",
            "Parameters: 9,452,544 (15.4%)\n",
            "FLOPs: 6,442,450,944 (16.7%)\n",
            "Memory: 36.06 MB (15.4%)\n",
            "Layers:\n",
            "  - attention_10\n",
            "    Dependencies: {'ffn_9', 'attention_10', 'ffn_10', 'attention_11'}\n",
            "  - ffn_10\n",
            "    Dependencies: {'ffn_9', 'attention_10', 'ffn_10', 'attention_11'}\n",
            "  - attention_11\n",
            "    Dependencies: {'ffn_9', 'attention_10', 'ffn_10', 'attention_11'}\n",
            "  - ffn_11\n",
            "    Dependencies: {'ffn_9', 'attention_10', 'ffn_10', 'attention_11'}\n",
            "\n",
            "Client 7:\n",
            "Parameters: 590,592 (1.0%)\n",
            "FLOPs: 589,824 (0.0%)\n",
            "Memory: 2.25 MB (1.0%)\n",
            "Layers:\n",
            "  - pooler\n",
            "    Dependencies: {'ffn_11'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF9V1tVNrP8e",
        "outputId": "ea1758fc-4f3c-4f35-a829-4c3afafe49b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    AlbertModel,\n",
        "    AlbertTokenizer,\n",
        "    AdamW\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Client0Config:\n",
        "    batch_size: int = 32\n",
        "    max_length: int = 128\n",
        "    learning_rate: float = 2e-5\n",
        "\n",
        "class MRPCDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: val[idx].clone().detach()\n",
        "            for key, val in self.encodings.items()\n",
        "        }\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "class Client0Trainer:\n",
        "    def __init__(self, config: Client0Config):\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "        self.model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "        self.embeddings = self.model.embeddings\n",
        "        self.config = config\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.embeddings.to(self.device)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Load MRPC dataset\n",
        "        dataset = load_dataset('glue', 'mrpc')\n",
        "        train_texts = list(zip(dataset['train']['sentence1'], dataset['train']['sentence2']))\n",
        "        train_labels = dataset['train']['label']\n",
        "\n",
        "        # Tokenize data\n",
        "        train_encodings = self.tokenizer(\n",
        "            train_texts,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.config.max_length,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        # Create custom dataset\n",
        "        self.train_dataset = MRPCDataset(train_encodings, train_labels)\n",
        "\n",
        "        # Create dataloader\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.config.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        # Forward pass through embeddings only\n",
        "        input_ids = batch['input_ids'].to(self.device)\n",
        "        token_type_ids = batch['token_type_ids'].to(self.device)\n",
        "\n",
        "        # Get embeddings output - ALBERT embeddings take input_ids and token_type_ids\n",
        "        outputs = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.embeddings.train()\n",
        "\n",
        "        for batch_idx, batch in enumerate(self.train_loader):\n",
        "            # Get embeddings output\n",
        "            OutputClient1 = self.train_step(batch)\n",
        "\n",
        "            # Save outputs periodically\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Processed batch {batch_idx}\")\n",
        "                # Save structure of the output for debugging\n",
        "                print(f\"Output shape: {OutputClient1.shape}\")\n",
        "                self.save_outputs(OutputClient1, batch_idx)\n",
        "\n",
        "    def save_outputs(self, outputs, batch_idx):\n",
        "        # Save outputs with batch index\n",
        "        output_path = f'client0_outputs_batch_{batch_idx}.pt'\n",
        "        torch.save(outputs, output_path)\n",
        "        print(f\"Saved outputs to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = Client0Config()\n",
        "    trainer = Client0Trainer(config)\n",
        "    trainer.prepare_data()\n",
        "    trainer.train_epoch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBB9NkkurNKY",
        "outputId": "50fa0663-2575-45eb-9e43-1157d7178f88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 0\n",
            "Output shape: torch.Size([32, 128, 128])\n",
            "Saved outputs to client0_outputs_batch_0.pt\n",
            "Processed batch 100\n",
            "Output shape: torch.Size([32, 128, 128])\n",
            "Saved outputs to client0_outputs_batch_100.pt\n"
          ]
        }
      ]
    }
  ]
}